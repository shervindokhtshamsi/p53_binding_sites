import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


# Step 1: Read the FASTA file and extract sequences
def read_fasta(file_path):
    sequences = []
    with open(file_path, 'r') as fasta_file:
        lines = fasta_file.readlines()

        sequence = ""
        for line in lines:
            line = line.strip()

            if line.startswith('>'):
                if sequence:
                    sequences.append(sequence)
                sequence = ""
            else:
                sequence += line

        if sequence:
            sequences.append(sequence)

    return sequences

# Step 2: Read the positive and negative sequence files
positive_file = 'C:/Users/user/Desktop/p53_project/positive_samples_complete.fasta'
negative_file = 'C:/Users/user/Desktop/p53_project/actin_samples_115.fasta'

positive_sequences = read_fasta(positive_file)
negative_sequences = read_fasta(negative_file)

# Step 3: Create a DataFrame for the sequences, with a label of 1 for positive samples and 0 for negative samples
positive_df = pd.DataFrame({'sequence': positive_sequences, 'label': 1})
negative_df = pd.DataFrame({'sequence': negative_sequences, 'label': 0})

# Combine the positive and negative DataFrames
data = pd.concat([positive_df, negative_df], ignore_index=True)

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['sequence'], data['label'], test_size=0.2, random_state=42)

# Step 5: Convert the DNA sequences into feature vectors using CountVectorizer
vectorizer = CountVectorizer(analyzer='char', ngram_range=(4, 4))  # Use 4-mers as features
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Step 6: Build and train the Logistic Regression model
model = LogisticRegression(max_iter=500)  # You can increase the max_iter value here
model.fit(X_train_vectorized, y_train)


# Step 7: Evaluate the model's performance
y_pred = model.predict(X_test_vectorized)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, output_dict=True)
support = report['weighted avg']['support']
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1_score = report['weighted avg']['f1-score']

# Step 8: Plot the results in one graph
results = [accuracy, precision, recall, f1_score, support]
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Support']

# Create subplots for metrics
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot metrics
axes[0].bar(metrics, results, color=['blue', 'green', 'red', 'purple', 'orange'])
axes[0].set_xlabel('Metrics')
axes[0].set_ylabel('Score')
axes[0].set_title('Model Evaluation Metrics')
axes[0].set_ylim(0, 1.2)

# Step 9: Plot the confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1])
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('True')
axes[1].set_title('Confusion Matrix')
axes[1].set_xticks([0, 1])
axes[1].set_yticks([0, 1])
axes[1].set_xticklabels(['Negative', 'Positive'])
axes[1].set_yticklabels(['Negative', 'Positive'])

plt.tight_layout()
plt.show()
